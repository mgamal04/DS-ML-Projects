{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e861dd9-9988-4f42-a320-573d4980ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LLaMA Streaming Response ---\n",
      "\n",
      "**Plain-English Summary:**\n",
      "This line of code generates a sequence (similar to a list or tuple) that contains the authors of all books in the `books` collection. The sequence is generated on-the-fly, rather than all at once, allowing for efficient processing of large datasets.\n",
      "\n",
      "**Breaking Down Syntax:**\n",
      "\n",
      "*   `{...}`: This is a dictionary comprehension syntax, which creates a new dictionary (in this case) based on the values obtained from the expression inside.\n",
      "*   `book.get(\"author\")`: This gets the value associated with the key `\"author\"` for each book in the `books` collection. If the key does not exist, it returns `None`.\n",
      "*   `{...}`: The outer dictionary comprehension iterates over the books that have an author (i.e., those where `book.get(\"author\") is not None`). For each such book, it adds its author to a new dictionary.\n",
      "*   `yield from`: This keyword yields control back to the caller of the function, allowing the generator to produce values instead of trying to return them all at once. It is like saying \"stop here and resume later\" to the code.\n",
      "\n",
      "**Reasoning Behind the Code:**\n",
      "\n",
      "The code uses a technique called **lazy evaluation**, where it only generates values when they are needed. This approach can be beneficial when working with large datasets, as it avoids the need to store all values in memory at once, which could lead to performance issues or even memory overflow.\n",
      "\n",
      "In this specific case, we're using the `yield from` keyword to delegate the creation of the author sequence to the dictionary comprehension inside. By doing so, we can take advantage of lazy evaluation and only generate authors for books that actually exist in the collection.\n",
      "\n",
      "Here's a more complete version of how you might use this code:\n",
      "\n",
      "```python\n",
      "def get_authors(books):\n",
      "    return yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "\n",
      "# usage:\n",
      "authors = get_authors([{\"title\": \"Book 1\", \"author\": \"Author 1\"}, {\"title\": \"Book 2\"}])\n",
      "for author in authors:\n",
      "    print(author)  # prints: Author 1\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "\n",
    "# Constants\n",
    "MODEL_GPT = \"gpt-4o-mini\"\n",
    "MODEL_LLAMA = \"llama3.2\"\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.startswith(\"sk-\") or len(api_key) < 10:\n",
    "    raise ValueError(\"Invalid or missing OpenAI API key.\")\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# ----- Helper Functions -----\n",
    "def build_messages(system_prompt: str, user_prompt: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Build the messages list for chat completion.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "# ----- Model Functions -----\n",
    "def stream_gpt_response(system_prompt: str, user_prompt: str, model: str = MODEL_GPT) -> None:\n",
    "    \"\"\"Stream response from GPT model.\"\"\"\n",
    "    messages = build_messages(system_prompt, user_prompt)\n",
    "    print(\"\\n--- GPT Streaming Response ---\\n\")\n",
    "    try:\n",
    "        stream = openai_client.chat.completions.create(model=model, messages=messages, stream=True)\n",
    "        for chunk in stream:\n",
    "            if chunk.choices and chunk.choices[0].delta.get(\"content\"):\n",
    "                print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error from GPT model: {e}]\")\n",
    "\n",
    "def stream_llama_response(system_prompt: str, user_prompt: str, model: str = MODEL_LLAMA) -> None:\n",
    "    \"\"\"Stream response from LLaMA model.\"\"\"\n",
    "    messages = build_messages(system_prompt, user_prompt)\n",
    "    print(\"\\n--- LLaMA Streaming Response ---\\n\")\n",
    "    try:\n",
    "        stream = ollama.chat(model=model, messages=messages, stream=True)\n",
    "        for chunk in stream:\n",
    "            if \"message\" in chunk and \"content\" in chunk[\"message\"]:\n",
    "                print(chunk[\"message\"][\"content\"], end=\"\", flush=True)\n",
    "        print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[Error from LLaMA model: {e}]\")\n",
    "\n",
    "# ----- Main -----\n",
    "if __name__ == \"__main__\":\n",
    "    # Static system prompt\n",
    "    system_prompt = (\n",
    "        \"You are an expert Python tutor who explains code step-by-step, \"\n",
    "        \"starting with a plain-English summary, then breaking down syntax, \"\n",
    "        \"and finally explaining the reasoning behind using that code structure. \"\n",
    "        \"Use clear examples and avoid jargon unless it is explained.\"\n",
    "    )\n",
    "\n",
    "    # Dynamic user prompt â€” just change the snippet to explain different code\n",
    "    code_snippet = 'yield from {book.get(\"author\") for book in books if book.get(\"author\")}'\n",
    "    user_prompt = f\"Please explain what this Python code does and why:\\n```python\\n{code_snippet}\\n```\"\n",
    "\n",
    "    # Stream responses\n",
    "    stream_gpt_response(system_prompt, user_prompt)\n",
    "    stream_llama_response(system_prompt, user_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed89a12c-d1da-4d04-90cc-9d1750e7b180",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
